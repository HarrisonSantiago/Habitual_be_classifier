{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Habitual Be Classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjibJnOZpgMRxJd+j4l7dO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarrisonSantiago/Habitual_be_classifier/blob/master/examples/Habitual_Be_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Setup**\n",
        "\n",
        "The following lines install the Habitual_be_classifier library in the Colab enviornment. "
      ],
      "metadata": {
        "id": "JEe4x8FsH8TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/HarrisonSantiago/Habitual_be_classifier.git\n",
        "\n",
        "% cd /content/Habitual_be_classifier \n",
        "! pip install numpy cython\n",
        "! pip install -e .\n",
        "\n",
        "import Habitual_be_classifier as hbc \n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "U5AETciYSaAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Rule Based Portion**\n",
        "\n",
        "The following cells load an example corpus which already has each instance of \"be\" labeled as habitual or non-habitual. The csv_processor splits these by default so that segments can be held out if desired. Here, all the data is then ran through the rule based filter. This filter removes as many non-habitual instances as possible, and returns the remaining undetermined instances. "
      ],
      "metadata": {
        "id": "7ux7ZyT_Iqem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = ['/content/Habitual_be_classifier/examples/CORAAL_example.csv']\n",
        "\n",
        "hab_input, nonhab_input = hbc.csv_processor(filepath)"
      ],
      "metadata": {
        "id": "Pg9qYx24owoM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = np.concatenate((hab_input, nonhab_input))"
      ],
      "metadata": {
        "id": "A9heX0hRFxSp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "declared_nonhab, unknown_hab = hbc.rule_filter(combined)"
      ],
      "metadata": {
        "id": "jYySFtuRF-Gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1d1fe9-6c9c-4c91-d304-b022cd72cf73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Augmentation** \n",
        "\n",
        "To run the augmentation functions, the word2 vec model must be downloaded and unzipped. The first cell shoes how to do that by downloading a copy of the word2vec model (provided by Tomas Mikolov). If the link has been accessed too frequently through gdown you may not be able to automatically download it. If so, just copy/paste the link into your browser, download from there, and run the gzip command. "
      ],
      "metadata": {
        "id": "arHs8ryjJkNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "!gzip -d GoogleNews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "id": "kZqT4ZIIG_Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_toStoreAugmentFiles = \".\"\n",
        "\n",
        "augmented = hbc.augmenter(unknown_hab, filepath_toStoreAugmentFiles)"
      ],
      "metadata": {
        "id": "LW9_JnHHGDQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Training the ML models**\n",
        "\n",
        "Here the augmented data and the data that the rule based filter could not classify is used to train the ML models. Then predicting the habituality of the same data is done. This lack of train/test split is not recommended in practice, but is done for demonstration purposes."
      ],
      "metadata": {
        "id": "Rw6HdgAKJtJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentedAndUnknown = np.concatenate((unknown_hab, augmented), axis = 0)\n",
        "\n",
        "y= augmentedAndUnknown[:,2].astype(int)\n",
        "\n",
        "X= hbc.vectorize(augmentedAndUnknown)\n",
        "\n",
        "models = hbc.algo_trainers(X, y)\n",
        "\n",
        "hab_prediction = models['ensemble'].predict(X)"
      ],
      "metadata": {
        "id": "9F8xILF3GPdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Using Pretrained Models**\n",
        "\n",
        "Here it is shown how to access the pretrained models that come with this package. Any classification with them is done in the same way as the models generated in Part 4."
      ],
      "metadata": {
        "id": "hoWvhcKkK0iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_models = hbc.get_pretrained('./Habitual_be_classifier/Classifiers.obj')\n",
        "\n",
        "print(pretrained_models.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt23TcPVKS4r",
        "outputId": "e27b2e22-09e9-4390-e076-4e5a9cf69c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Logistic Regression', 'Linear SGD Classifier', 'Neural Net', 'Ensemble'])\n"
          ]
        }
      ]
    }
  ]
}